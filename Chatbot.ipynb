{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"_3NRRbFSO6e_"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-03-25 12:17:28.309294: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-25 12:17:28.431194: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2023-03-25 12:17:28.431215: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","2023-03-25 12:17:29.160293: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","2023-03-25 12:17:29.160359: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","2023-03-25 12:17:29.160367: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","2023-03-25 12:17:30.290922: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n","2023-03-25 12:17:30.290946: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n","2023-03-25 12:17:30.290965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (XXIII): /proc/driver/nvidia/version does not exist\n","2023-03-25 12:17:30.291145: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]},{"ename":"NameError","evalue":"name 'pad_sequences' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/home/person_xxiii/Documents/FYP/Al-Siraj : An Islamic Chatbot/Trained_Model/Trained model.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/person_xxiii/Documents/FYP/Al-Siraj%20%3A%20An%20Islamic%20Chatbot/Trained_Model/Trained%20model.ipynb#W0sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m prediction_input \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtexts_to_sequences(texts_p)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/person_xxiii/Documents/FYP/Al-Siraj%20%3A%20An%20Islamic%20Chatbot/Trained_Model/Trained%20model.ipynb#W0sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m prediction_input \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(prediction_input)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/person_xxiii/Documents/FYP/Al-Siraj%20%3A%20An%20Islamic%20Chatbot/Trained_Model/Trained%20model.ipynb#W0sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m prediction_input \u001b[39m=\u001b[39m pad_sequences([prediction_input],maxlen\u001b[39m=\u001b[39m\u001b[39m18\u001b[39m) \u001b[39m# keep maxlen equal to at what the model was trained at. in model it is as 'input_shape'.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/person_xxiii/Documents/FYP/Al-Siraj%20%3A%20An%20Islamic%20Chatbot/Trained_Model/Trained%20model.ipynb#W0sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# get the output from the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/person_xxiii/Documents/FYP/Al-Siraj%20%3A%20An%20Islamic%20Chatbot/Trained_Model/Trained%20model.ipynb#W0sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(prediction_input)\n","\u001b[0;31mNameError\u001b[0m: name 'pad_sequences' is not defined"]}],"source":["import tensorflow as tf\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","import json\n","import string\n","import random\n","\n","# load the tokenizer and label encoder\n","with open('Saved Files/tokenizer.json') as f:\n","    data = json.load(f)\n","    tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n","\n","with open('Saved Files/label_encoder.json') as f:\n","    data = json.load(f)\n","    le = LabelEncoder()\n","    le.classes_ = np.array(data)\n","\n","with open('Saved Files/responses.json') as file:\n","    responses = json.load(file)\n","\n","# load the trained model\n","model = tf.keras.models.load_model('Saved Files/my_model.h5')\n","\n","# start the chatbot\n","while True:\n","    # get user input\n","    texts_p = []\n","    prediction_input = input('You: ')\n","    \n","    # preprocess the input\n","    prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]\n","    prediction_input = ''.join(prediction_input)\n","    texts_p.append(prediction_input)\n","    prediction_input = tokenizer.texts_to_sequences(texts_p)\n","    prediction_input = np.array(prediction_input).reshape(-1)\n","    prediction_input = pad_sequences([prediction_input],maxlen=18) # keep maxlen equal to at what the model was trained at. in model it is as 'input_shape'.\n","\n","    # get the output from the model\n","    output = model.predict(prediction_input)\n","    output = output.argmax()\n","    \n","    # find the tag and select a response\n","    response_tag = le.inverse_transform([output])[0]\n","    response = random.choice(responses[response_tag])\n","    \n","    # print the response\n","    print('Bot:', response)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOZFbNFky/RQblRu82tBHqO","mount_file_id":"1wVOfBUxIlQyAnmvueWcWH2cnAjiG9uM4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
